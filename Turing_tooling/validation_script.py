"""
Validation Script for OSWorld Task Deliveries

This script validates the structure and content of a task delivery folder
based on specified criteria including JSON schema validdef check_annotator_scores(task_folder):
    scores = []
    for i in range(1, 4):
        score_file = os.path.join(task_folder, 'Annotator_trajectory', f'annotaor_{i}', 'evaluation_score.txt')
        if not os.path.exists(score_file):
            print(f"‚ùå Annotator scores check: evaluation_score.txt not found for annotator {i}.")
            return False
        with open(score_file, 'r') as f:
            try:
                val = int(f.read().strip())
                scores.append(val)
            except ValueError:
                print(f"‚ùå Annotator scores check: Invalid score for annotator {i}.")
                return False
    count_1 = scores.count(1)
    count_0 = scores.count(0)
    print(f"üìä Annotator scores: {count_1} passed (1), {count_0} failed (0).")
    if 1 not in scores or 0 not in scores:
        print("‚ùå Annotator scores check: Scores do not include both 1 and 0.")
        return False
    print("‚úÖ Annotator scores check: PASSED")
    return Truere,
evaluation scores, and more.

Usage:
    python Turing_tooling/validation_script.py <delivery_folder> <task_id> [--checks CHECKS]

Arguments:
    delivery_folder: Path to the Deliverable folder (e.g., e:\OSWorld\Deliverable)
    task_id: The task ID (e.g., libreoffice-europe-countries-task-1)

Options:
    --checks: Comma-separated list of checks to run. Available checks:
        - json: Validate JSON structure against schema
        - evaluator_diff: Check evaluator diff for new functions
        - passk: Validate Pass@k values from Claude runs
        - structure: Check required file structure
        - sft_score: Validate SFT evaluation score
        - annotator_scores: Validate annotator evaluation scores
        - no_args: Ensure no args.json files are present
        Default: all checks

Examples:
    python Turing_tooling/validation_script.py e:\OSWorld\Deliverable task_123
    python Turing_tooling/validation_script.py e:\OSWorld\Deliverable task_123 --checks json,structure
"""

import os
import json
import jsonschema
import argparse
from jsonschema import validate, ValidationError

def load_schema(schema_path):
    with open(schema_path, 'r') as f:
        return json.load(f)

def validate_json_structure(task_json_path, schema):
    try:
        with open(task_json_path, 'r') as f:
            task_data = json.load(f)
        validate(instance=task_data, schema=schema)
        print("‚úÖ JSON structure validation: PASSED")
        return True, task_data
    except ValidationError as e:
        print(f"‚ùå JSON validation error: {e.message}")
        return False, None
    except Exception as e:
        print(f"‚ùå Error loading JSON: {e}")
        return False, None

def check_evaluator_diff(task_data, task_folder):
    evaluator_funcs = task_data.get('evaluator', {}).get('func', [])
    if isinstance(evaluator_funcs, str):
        evaluator_funcs = [evaluator_funcs]
    
    schema_funcs = [
        "exact_match", "is_expected_tabs", "is_expected_active_tab", "is_expected_active_tab_approximate",
        "is_added_to_steam_cart", "check_direct_json_object", "is_expected_bookmarks", "is_shortcut_on_desktop",
        "is_expected_url_pattern_match", "infeasible", "check_history_deleted", "check_enabled_experiments",
        "is_in_list", "is_cookie_deleted", "check_font_size", "match_in_list", "compare_pdfs",
        "check_palette_and_structure_sim", "check_structure_sim", "check_saturation_increase_and_structure_sim",
        "check_image_mirror", "check_green_background", "check_config_status", "check_file_exists_and_structure_sim",
        "check_brightness_decrease_and_structure_sim", "check_include_exclude", "check_image_size",
        "check_structure_sim_resized", "check_textbox_on_leftside", "check_triangle_position",
        "check_contrast_increase_and_structure_sim", "compare_table", "compare_csv", "check_pdf_pages",
        "compare_pptx_files", "check_presenter_console_disable", "check_transition", "check_auto_saving_time",
        "evaluate_presentation_fill_to_rgb_distance", "compare_images", "check_image_stretch_and_center",
        "check_page_number_colors", "compare_audios", "check_slide_orientation_Portrait", "check_left_panel",
        "compare_line_spacing", "check_tabstops", "compare_docx_files", "compare_subscript_contains",
        "has_page_numbers_in_footers", "compare_font_names", "is_first_line_centered", "compare_docx_tables",
        "check_highlighted_words", "compare_docx_images", "compare_unique_train_records",
        "evaluate_strike_through_last_paragraph", "evaluate_colored_words_in_tables", "check_italic_font_size_14",
        "contains_page_break", "find_default_font", "compare_numbered_lists", "compare_image_text",
        "compare_archive", "compare_text_file", "file_contains", "check_line_number", "check_python_file_by_test_suite",
        "compare_references", "check_image_file_size", "check_mp3_meta", "check_list", "compare_epub",
        "is_extension_installed", "compare_image_list", "compare_config", "literal_match",
        "compare_docx_files_and_ignore_new_lines", "compare_conference_city_in_order", "is_in_vm_clickboard",
        "check_json", "diff_text_file", "check_thunderbird_folder", "compare_result_files", "fuzzy_place_math",
        "is_expected_installed_extensions", "compare_zip_files", "compare_pdf_images", "compare_python_pure_text",
        "compare_htmls", "check_accessibility_tree", "is_expected_search_query", "check_moved_jpgs", "is_utc_0",
        "check_gnome_favorite_apps", "check_thunderbird_prefs", "check_csv", "check_thunderbird_filter",
        "run_sqlite3", "check_qt_bgcone", "check_global_key_play_pause", "is_vlc_playing",
        "is_vlc_recordings_folder", "is_vlc_fullscreen", "check_qt_max_volume", "check_qt_minimal_view",
        "compare_videos", "check_qt_slider_colours", "check_one_instance_when_started_from_file",
        "check_json_settings", "check_json_keybindings"
    ]
    
    new_funcs = [f for f in evaluator_funcs if f not in schema_funcs]
    if new_funcs:
        diff_path = os.path.join(task_folder, 'SFT', 'evaluator.diff')
        if not os.path.exists(diff_path):
            print(f"‚ùå Evaluator diff check: New evaluator functions {new_funcs} found, but evaluator.diff not present.")
            return False
        with open(diff_path, 'r') as f:
            diff_content = f.read()
        for func in new_funcs:
            if func not in diff_content:
                print(f"‚ùå Evaluator diff check: Function {func} not defined in evaluator.diff")
                return False
    print("‚úÖ Evaluator diff check: PASSED")
    return True

def validate_pass_k(task_folder, task_id):
    claude_folder = None
    for item in os.listdir(task_folder):
        if item.startswith('claude') and 'log' in item:
            claude_folder = os.path.join(task_folder, item)
            break
    if not claude_folder:
        print("‚ùå Pass@k validation: Claude folder not found.")
        return False
    
    results = []
    for i in range(1, 17):
        run_folder = os.path.join(claude_folder, f'run_{i}')
        if not os.path.exists(run_folder):
            print(f"‚ùå Pass@k validation: Run folder {run_folder} not found.")
            return False
        result_file = os.path.join(run_folder, 'Trajectory and Screenshot', task_id, 'result.txt')
        if not os.path.exists(result_file):
            print(f"‚ùå Pass@k validation: result.txt not found in {result_file}")
            return False
        with open(result_file, 'r') as f:
            try:
                val = int(f.read().strip())
                if val not in [0, 1]:
                    print(f"‚ùå Pass@k validation: Invalid value in {result_file}: {val}")
                    return False
                results.append(val)
            except ValueError:
                print(f"‚ùå Pass@k validation: Non-integer value in {result_file}")
                return False
    num_runs = len(results)
    print(f"üìä Pass@k validation: Found {num_runs} runs.")
    if num_runs != 16:
        print("‚ùå Pass@k validation: Not exactly 16 runs.")
        return False
    avg = sum(results) / 16
    if avg == 0 or avg == 1:
        print(f"‚ùå Pass@k validation: Average is {avg}, which is invalid (must be between 0 and 1).")
        return False
    print(f"‚úÖ Pass@k validation: PASSED with average {avg}")
    return True

def check_file_structure(task_folder, task_id):
    required = [
        f'{task_id}.json',
        'SFT/libreoffice_writer/' + task_id + '/evaluation_score.txt',
        'Annotator_trajectory/annotaor_1/Trajectory and Screenshot',
        'Annotator_trajectory/annotaor_1/evaluation_score.txt',
        'Annotator_trajectory/annotaor_2/Trajectory and Screenshot',
        'Annotator_trajectory/annotaor_2/evaluation_score.txt',
        'Annotator_trajectory/annotaor_3/Trajectory and Screenshot',
        'Annotator_trajectory/annotaor_3/evaluation_score.txt'
    ]
    for req in required:
        path = os.path.join(task_folder, req)
        if not os.path.exists(path):
            print(f"‚ùå File structure check: Required file/folder missing: {path}")
            return False
    print("‚úÖ File structure check: PASSED")
    return True

def check_sft_eval_score(task_folder, task_id):
    score_file = os.path.join(task_folder, 'SFT', 'libreoffice_writer', task_id, 'evaluation_score.txt')
    if not os.path.exists(score_file):
        print("‚ùå SFT eval score check: evaluation_score.txt not found in SFT.")
        return False
    with open(score_file, 'r') as f:
        try:
            val = int(f.read().strip())
            if val != 1:
                print(f"‚ùå SFT eval score check: Score is {val}, not 1.")
                return False
        except ValueError:
            print("‚ùå SFT eval score check: Invalid score value.")
            return False
    print("‚úÖ SFT eval score check: PASSED")
    return True

def check_annotator_scores(task_folder):
    scores = []
    for i in range(1, 4):
        score_file = os.path.join(task_folder, 'Annotator_trajectory', f'annotaor_{i}', 'evaluation_score.txt')
        if not os.path.exists(score_file):
            print(f"‚ùå Annotator scores check: evaluation_score.txt not found for annotator {i}.")
            return False
        with open(score_file, 'r') as f:
            try:
                val = int(f.read().strip())
                scores.append(val)
            except ValueError:
                print(f"‚ùå Annotator scores check: Invalid score for annotator {i}.")
                return False
    count_1 = scores.count(1)
    count_0 = scores.count(0)
    print(f"üìä Annotator scores: {count_1} passed (1), {count_0} failed (0).")
    if 1 not in scores or 0 not in scores:
        print("‚ùå Annotator scores check: Scores do not include both 1 and 0.")
        return False
    print("‚úÖ Annotator scores check: PASSED")
    return True

def check_no_args_json(task_folder):
    for root, dirs, files in os.walk(task_folder):
        if 'args.json' in files:
            print(f"‚ùå No args.json check: args.json found in {root}")
            return False
    print("‚úÖ No args.json check: PASSED")
    return True

def main(delivery_folder, task_id, selected_checks):
    schema_path = os.path.join(delivery_folder, '..', 'osworld_updated_schema.json')
    if not os.path.exists(schema_path):
        print("‚ùå Schema file not found.")
        return
    schema = load_schema(schema_path)
    
    task_folder = os.path.join(delivery_folder, task_id)
    if not os.path.exists(task_folder):
        print(f"‚ùå Task folder {task_folder} not found.")
        return
    
    task_json_path = os.path.join(task_folder, f'{task_id}.json')
    if not os.path.exists(task_json_path):
        print(f"‚ùå Task JSON {task_json_path} not found.")
        return
    
    valid, task_data = validate_json_structure(task_json_path, schema) if 'json' in selected_checks else (True, None)
    if not valid and 'json' in selected_checks:
        return
    
    # Load task_data if needed for other checks
    if any(check in selected_checks for check in ['evaluator_diff', 'passk', 'structure', 'sft_score', 'annotator_scores', 'no_args']):
        if task_data is None:
            try:
                with open(task_json_path, 'r') as f:
                    task_data = json.load(f)
            except Exception as e:
                print(f"‚ùå Error loading task JSON: {e}")
                return
    
    check_functions = {
        'evaluator_diff': lambda: check_evaluator_diff(task_data, task_folder),
        'passk': lambda: validate_pass_k(task_folder, task_id),
        'structure': lambda: check_file_structure(task_folder, task_id),
        'sft_score': lambda: check_sft_eval_score(task_folder, task_id),
        'annotator_scores': lambda: check_annotator_scores(task_folder),
        'no_args': lambda: check_no_args_json(task_folder)
    }
    
    results = []
    passed_count = 0
    total_checks = len(selected_checks)
    if 'json' in selected_checks:
        results.append(valid)
        if valid:
            passed_count += 1
    
    for check_name, check_func in check_functions.items():
        if check_name in selected_checks:
            result = check_func()
            results.append(result)
            if result:
                passed_count += 1
    
    if all(results):
        print("üéâ All selected validations PASSED!")
    else:
        print("‚ö†Ô∏è  Some selected validations FAILED.")
    print(f"üìà Validation Summary: {passed_count}/{total_checks} checks passed.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Validate OSWorld task delivery")
    parser.add_argument("delivery_folder", help="Path to the Deliverable folder")
    parser.add_argument("task_id", help="The task ID")
    parser.add_argument("--checks", default="all", help="Comma-separated list of checks to run (default: all)")
    
    args = parser.parse_args()
    
    if args.checks == "all":
        selected_checks = ['json', 'evaluator_diff', 'passk', 'structure', 'sft_score', 'annotator_scores', 'no_args']
    else:
        selected_checks = [check.strip() for check in args.checks.split(',')]
        available_checks = ['json', 'evaluator_diff', 'passk', 'structure', 'sft_score', 'annotator_scores', 'no_args']
        invalid_checks = [check for check in selected_checks if check not in available_checks]
        if invalid_checks:
            print(f"‚ùå Invalid checks: {invalid_checks}")
            print(f"Available checks: {available_checks}")
            exit(1)
    
    main(args.delivery_folder, args.task_id, selected_checks)

